{"pages":[{"title":"","text":"呀，一些碎碎念做一个自己的网站很酷呀，不过做完就去找工作了。（找工作倒是写了很多笔记，可以记录上来哈哈）但是也耽误了博客的更新，想起来可以记录些自己的想法，见闻，技术内容也挺好的。我一定好好更新嘻~~ 关于我：一个兜兜转转到CV的非典型程序媛 专业相关：目前在做计算机视觉相关的项目和研究，主要目标检测和语义分割做的多一些，目前在研究基于语义分割的领域自适应工作，争取出些成果（加油~） 业余爱好：喜欢各种运动（羽毛球，网球，足球，篮球，都很喜欢）；日日离不开耳机；一看美剧就人间消失；看展看演出积极分子； 经历：焊板子，单片机编程，matlab仿真，计算机视觉都做过哈哈，虽然前几项已经忘得差不多了。","link":"/about/index.html"}],"posts":[{"title":"使用字符串","text":"这一节，利用字符串读取和输出的程序，了解字符串的相关操作，常量，和变量的声明以及初始化等概念。 学习要点： 变量和对象的区分 链式输入和输出缓存区 字符类型的操作，运算符重载等 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;string&gt;int main(){ std::cout &lt;&lt; \"please enter your first name: \"; std::string name; std::cin &gt;&gt; name; const std::string greetings = \" Hello, \" + name + \"! \"; const std::string spaces(greetings.size(),' '); const std::string second = \"*\" + spaces + \"*\"; const std::string first(greetings.size()+2,'*'); std::cout &lt;&lt; std::endl; std::cout &lt;&lt; first &lt;&lt; std::endl; std::cout &lt;&lt; second &lt;&lt; std::endl; std::cout &lt;&lt; '*' &lt;&lt; greetings &lt;&lt; '*' &lt;&lt; std::endl; std::cout &lt;&lt; second &lt;&lt; std::endl; std::cout &lt;&lt; first &lt;&lt; std::endl; return 0;} 变量和对象在使用 std::cin 读取输入之前，我们使用了标准库种的 std::string 类来声明了一个字符串变量，用于存储 std::cin 的内容。 什么是变量？ 变量和对象？变量是有名称的一个对象，对象则是计算机内具有类型的一段存储空间，对象的类型决定了可以对对象进行的操作和操作的结果。变量和对象之间通过两种方式进行区分： 变量是一种有名称的对象，这样可以用来被编译器检测是否存在命名错误 变量存在事件有限：在花括号中声明的变量的存活时间限于}，之后就被销毁和回收，这就是一种局部变量 除了区分点，变量继承了指定对象接口中的所有操作，如我们下面列举的 std::string 类型的操作就可以被任何 std::string 类型的变量使用。 字符变量的声明和操作字符变量的声明有四种形式： 直接声明，并隐式初始化为空字符串，对于其他类型则会初始化为其他的值1std::string name; // name = '' 声明的同时赋值初始化1std::string name = \"a string\"; //name = \"a string' 使用方法来重复字符直接量来构造字符：系统环境会根据表达式构造一个变量。其中字符直接量的类型是内建类型char，字符串的内建类型则复杂得多1std::string spaces(name.size(),' '); // spaces = ' ' 使用const 关键字，建立常量变量，其在变量存在的时间内不会改变数值，并且需要在创建时就赋值初始化，否则之后不会有改变的机会；初始化常量时，我们也可以使用变量进行初始化1const std::string name = \"I am a const\" + name; 处理初始化操作之外，我们还可以对为std::string 类型的变量进行一些其他的操作： 输入字符到当前变量：输入字符时会忽略输入头部的空格，tab，换行符等，并持续读入非空白输入，直到遇到下一个空格，接下来的输入归入到下一次输入，并返回std::cin，因此我们可以使用其进行链式输入1234std::string name1;std::string name2;std::cin &gt;&gt; name1;std::cin &gt;&gt; name1 &gt;&gt; name2; // 链式输入 将结果写入到输出流：返回输出流12std::string name;std::cout &lt;&lt; name; 将字符写入输出流时，其实我们是将其写入了缓冲区，缓冲区的设计是为了优化多次频繁输出开销很多的设计。在运行一个时间复杂度较高的程序时，我们需要时常刷新缓存区来获得及时的输出。 读取输入：在读取输入时，缓存区内容就必须先输出然后再读取输入 控制器刷新：使用std::endl可以进行缓存区的刷新 缓存区满：当缓存区满了会自动刷新 获取size: 可以读取字符串中字符的个数，这里只包含非空字符的个数12std::string name;int size = name.size() 使用“+”和字符串或者字符串直接量相连接：其中+字符和在3+4表达式中有不同的含义，这样的情况我们就称为运算符重载12std::string name = 'string';std::string = \"a\" + name;","link":"/2020/09/16/CPP/%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"title":"开始学习C++","text":"本章开始，通过阅读《Accelerated C++》开始记录笔记并学习学习要点： 标准库和其代表的名字空间 表达式：被操作数和运算符组成了一个表达式，其中运算符有左结合/右结合的性质，被操作数则是由其类型决定表达式的结果 作用域：学习了两种作用域的生成方式，分别是花括号和名字空间 12345678#include &lt;iostream&gt;//这是一个简单的cpp程序int main(){ std::cout &lt;&lt; \"Hello, World!\" &lt;&lt;std::endl; return 0;} 在这个简单的程序中，我们将学习到表达式，作用域，运算符，作用数等一系列的概念 1. 注释：可以使用 // 进行单行注释，也可以使用/*/ 来进行多行注释（每次跨行需要行首加上 ），当使用// 时，其优先级会高于多行注释 2. include：使用 include 语句来包含不属于语言核心的标准库来增加对额外的指出 3. main 函数：每一个 cpp 程序都需要包含且只能包含一个 main 函数，编译完成后，程序自行调用 main 函数，main 函数通常返回 0 来表示成功。 函数的返回值类型：函数定义的初始制定函数返回值的类型，每个返回值都需要满足这个类型要求 函数的参数：在函数名后的小括号中可以传入函数的参数，其中 main 函数也可以传入参数 花括号：函数的花括号代表了函数体，其中所有语句当成一个单元来处理 4. 标准库输入输出：1std::cout &lt;&lt; \"Hello World\" &lt;&lt; std::endl; 这里使用了输出运算符 &lt;&lt; 来将字符串直接量和控制器 std::endl 写入到 std::cout 标准输出流中，这是一种链式输出。 std 名字空间：是所有标准库中变量所在的空间，通过作用域运算符 ::，我们就可以在名字空间中找到对应的变量，而这个变量的作用域就是这个名字空间 输出运算符号 &lt;&lt;: 用于将变量不断写入输出流 控制器：控制器 std::cout 用于控制数据流，并返回流作为其结果 5. 表达式：表达式由操作数和运算符组成 操作数：每一个操作数都有一个类型，代表了数据结构和对这个数据结构合理的操作，表达式的结果取决于草所数的类型。 运算符：用于运算被操作数的实体，其中 &lt;&lt; 输出运算符是左结合的，也就是运算符左边的操作数由尽可能多地结合左边的表达式生成，右边的操作数则是尽可能结合少的表达式的结果，也就是会产生从左到右依次执行的结果 6. 作用域：变量的作用域代表变量只有在这个函数部分中才有含义，这里有两种定义域： 名字空间： 通过作用域运算符，我们可以在指定名字空间中找到我们想要的变量，防止了名字的冲突 花括号：定义函数主体，一般在函数中定义的变量，作用域在于函数体中","link":"/2020/09/12/CPP/%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0cpp/"},{"title":"见闻","text":"","link":"/2020/09/09/%E6%89%80%E8%A7%81%E6%89%80%E9%97%BB/post/"},{"title":"思考","text":"","link":"/2020/09/09/%E6%97%A5%E6%9C%89%E6%89%80%E6%80%9D/post/"},{"title":"CAG_UDA","text":"Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation中心思想：核心思想为基于源域类别 Anchor 的分布对齐，实现两个域之间类内距离减小，类间距离增大的目的，更加利于生成分界面，同时使用对目标与分配伪标签的方式促使分界面不从数据中心穿过，也减少分类器对源域的偏爱。 类别层次的特征对齐：基于源域和目标域相同类别的特征向量在特征空间中距离较近的假设，把源域的每个类别上计算类别的平均值当成是类别中心，并促使源域的同一类别特征向量和目标域的激活特征向量向类别中心靠拢。 提升模型泛化能力：基于源域 Anchor 给激活的目标域特征分配伪标签，分类器使用伪标签进行训练，促使分类边界也根据目标域的标签进行相应的调整。 这个方法可以解决全局特征对齐带来的类别没有对齐的问题，通过向激活向量，即在目标域中原有特征空间中和源域某一个类别中心较为接近，和其他类别中心的距离均超过一个设置的门限的特征向量，分配对应 cluster 的伪标签，实现两个域类内距离减少。 主要步骤： 建立源域类别Anchor（CAC）： 1.由于源域的图片都具有标签，那么，在fD之后的特征向量上，筛选出相同类别的特征向量（维度：num_channel 1 1）,并计算均值得到类别Anchor—fcs 根据类别Anchor明确目标域中的active target instance(ATI)： 1.active target instance的选择方式即目标域中的fD之后的特征向量和某个fsc的距离较近，和其他fsc的距离的差值都超过了一个设定的门限（这个门限需要人为设定优点僵硬，其实可以换成比例这样的形式？） 给active target instance分配伪标签(PLA)： 1.active target instance的标签就是距离最近的那个类别 建立距离和分割损失约束，进行分布对齐和分类边界优化。 首先，需要建立一个适应源域特征的好的分割模型，从而基于源域的信息迁移到目标域。即先在源域上对分类器上进行训练，这里涉及了源域的分割损失： 对源域和目标域的分布进行一个整体对齐的操作：可以有多种多样选择，判别器对抗训练，输入风格迁移等 采用stage的方式训练。每个stage： 约束源域的sample都尽量靠近对应类别的anchor： 约束分类边界和源域贴合 约束目标域的active sample贴合源域的Anchor，从而目标域细节分布靠近源域 约束目标域伪标签下的分割损失，从而分类边界依据目标域对齐后的active sample进行调整 优势： 清晰明了的方式说明如何进行类内对齐并通过判别器损失提升类间距离，整体流程很自然。 使用anchor的形式，相较于使用分类器的预测结果获得伪标签而言，减少了分类器的bias，可以对比由概率生成的伪标签，可以看出其能更加正确生成行人等标签，是一种互补的形式 在目标域通过伪标签的形式加强了监督，提升了效果 Limitations： 目标域和源域的数据分布假设强烈：源域和目标域相同类别的特征向量在特征空间中距离较近的假设过于强烈，尤其是对于源域和目标域原本相差较大的情况下，上述对齐方法如作者在文中提到的实际上会得到更差的结果。因此需要先对源域和目标域进行一个基础对齐工作，才能进行类内对齐，即需要一种coarse-to-fine的特征对齐方式。 非激活向量的处理：文中仅仅对激活向量进行了操作，而没有考虑目标域中的非激活向量，尽管作者说非激活向量会随着激活向量对齐的同时同样地对齐，但是说服力较低，也没有一个严谨地阐述，因此，我们需要对非激活向量进一步操作，例如进行熵最小话这样的操作。 像素代表实体：作者使用一个基于像素点的均值来代表全体像素点，但是像素的信息相对于一个语义实体来说，有点过于省略，也许我们可以使用多个聚类中心来代表语义实体，来促使anchor更加具有代表行。另外还可以思考如何利用大大小小类别的结构信息， 类别不均衡：由于距离损失和交叉熵损失中还是以像素为单位进行累计，因此还是可能会存在类别不均衡问题","link":"/2020/09/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CAG_UDA/"},{"title":"Bidirectional Learning for Domain Adaptation of Semantic Segmentation","text":"主要思想在图像变换和分割网络中融入perception loss 来减少不同特征对分割网络的影响，随后使用双向学习和自监督学习提升网络的泛化能力，并使得两阶段网络不断互相促进。其中双向学习使得图像变换网络和分割网络不断迭代更新，相互促进优化，自监督学习使用分类器输出结果给目标域图片分配伪标签来约束分割网络 流程步骤 建立三大损失来约束 Cycle GAN: 变换结果不影响语义分割的感知：将S和T分别变换之后经过分割网络获取的特征用于计算感知loss，即迁移的结果在分割网络的特征提取下在感知loss衡量下相近，即特征相近 GANloss：迁移后S’和T的分布相近 reconloss：迁移后再迁移回来，信息没有损失 使用cycle GAN 进行源域利用风格迁移到目标域，并用分割损失和对抗损失进行模型约束和特征对齐。 模型约束：迁移后的图片使用分割网络利用标签信息进行分割 特征对齐：同时增加对抗损失促使变换后的源域特征和目标域的一致。 使用分割网络的概率结果，生成自监督学习伪标签 模型约束：分割模型在输出较大概率的当成是图像的伪标签，从而使用这个伪标签进行图像约束，在对模型的约束上增加了一层 由前三步组成两层循环进行训练 外层循环主要训练好变换网络，并用源域信息和特征对齐损失监督分割模型 内层循环使用不断优化的分割网络来生成更多的伪标签来进行模型的监督。伪标签的生成使用门限进行筛选，概率超过门限的才计入损失当中 讨论： 作者通过消融实验证明，不断迭代进行多stage 的训练有利于精度的提升 同时引入SSL 能对精度有较大的提升。7%左右 在分配伪标签时，门限对于标签的影响较大，一开始较高的门槛有助于筛选不正确的标签，提升效果，随后更高的门槛则会减少有利的信息，导致网络难以学习。 优势： 每周期重置了伪标签，防止了错误的累计 双向学习有利于互相促进 Limitations： 目标域和源域特征相似，同时源域变换前后的特征也一致：特征对齐中对特征的相似性要求过多，同时对抗损失，即源域和目标域的特征对齐上没有精细到类别层次 网络结构太大，不利于反向传播","link":"/2020/09/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/BDL/"},{"title":"领域自适应综述","text":"温故知新篇 — 领域自适应中的多种方法和内部含义浅析本文的行文结构如下，首先从定义出发描述，继而通过现有问题和数学模型开始理解什么是领域自适应问题。基于理解，我们罗列现有的几大主流领域自适应方法，并使用我们的数学模型辅助理解，提出笔者的一些思考。 领域自适应定义：现有深度学习模型使用源域的知识，并运用一系列领域自适应方法，提升其在目标域上的表现。基于研究领域自适应问题本质为研究泛化问题，其有众多的应用方向，包括但不限于分类，目标检测，语义分割等。 如何理解领域自适应： 首先，我们可以来看一下，什么样的情况会导致领域不适应；接着，我们来使用自己建立的数学模型来理解领域不适应时到底发生了什么；最后我们引出，基于这个数学模型，我们如何来理解现有的多种领域自适应方法。 领域不适应的情形：领域不适应，即神经网络的泛化性能较差，其在以下情形容易发生： 神经网络模型对于训练集分布过拟合：网络在训练集上的指标和测试集上指标差异达到5%及以上，并且随着训练的进行差异越来越大。 测试集分布和训练集分布差异较大：我们发现，在生成图像上训练的模型，在真实图片上训练效果很差。 这时，我们会说神经网络在测试集上的表现性能差，我们称这个神经网络模型的泛化性能差。那么，如何从数学模型的角度来理解泛化的问题呢？ 数学模型解释泛化： 我们不妨先假设我们在进行分类问题，同时我们将数据的分布降为两维，而不是图像的千万维，那么分类问题即在二维平面上寻找一条分界线，将两个不同的标记的类别进行区分。 神经网络对于训练集过拟合：这个情形我们应当很熟悉了，此时分界线在训练集的不同类别之间交叉穿过，但是由于测试集的分布和训练集不同，因此产生了大量的错误分类。这时，我们的问题在于分界线的复杂度太高，超过了数据本身真实分布需要的分界线，或者可以说模型的复杂度和真实分界线的复杂度之间有维度差距。 测试集和训练集分布差异大：这时我们可以想象一个适当复杂度的分界线对训练集的不同类别进行分类，但是测试集和训练集的分布有差异。同样的类别，但是产生了分布偏移，甚至和原来的类别差异较远。说明当前用于训练神经网络模型的数据分布不能反映真实数据的分布。从而生成的分界线不能正确分割真实数据。 从以上分析我们可以看出，领域自适应的问题，本质就是数据分布和分界线的问题，一个良好的领域自适应方法应当同时兼顾对齐源域和目标域的数据分布，同时对分界线进行约束。从而源域和目标域的数据分布对齐良好，同时模型的分界边界远离数据分布并能泛化到其他相同分布数据。 现有的多种解决领域自适应的方法： 基于A review of domain adaptation without target labels [1] 这篇综述，我们按照下面三个方向进行分类： 基于采样的方法: 给予不同样本不同的权重，促使源域中分布和目标域不一致的部分尽量少地影响神经网络。这类方法不能对某一类数据的分布进行变换（平移，聚集等），从而不能优化源域和目标域数据同一类别特征有较大差异带来的影响，而只能通过改变分布的存在与否来间接地影响分类边界。 基于特征对齐的方法: 将源域和目标域的神经网络特征分布进行对齐，从而同样类别的物体能在特征分布空间中尽量地接近。这类方法应当是最灵活的，但是我们需要找到一个合适的方式来实现特征对齐。 基于推理的方法：通过在优化过程中加上额外限制的方式，从而估计神经网络的模型参数，促使神经网络学习的分界线满足限制。这类方法可以看成是对分界线的变换下面我们通过独立的模块，细致地描述这些方法 如何做到领域自适应做到领域自适应，关键在于如何基于源域数据上学习到的分界线信息，加上源域和目标域之间的差异信息，促使数据差异减小，分界线向更适应目标域的方向移动。 1. 基于采样的方法：基于采样的方法目的在于消除采集源域数据时的偏差带给神经网络的影响，我们希望采样的数据分布和真实的数据分布尽量一致，从而提升神经网络对真实数据的适应性。现代的数据方法往往通过预估真实分布中每个实例出现的比例来进行采样后分布的校正，但是在真实场景的数据往往没那么好估计。 数据层次重要性权重 类别层次重要性权重2. 基于特征对齐的方法： 子空间映射 最佳传输 域不变空间匹配 深度特征匹配 协同学习（Correspondence learning） 参考文献： Kouw, W.M., &amp; Loog, M. (2019). A review of single-source unsupervised domain adaptation. ArXiv, abs/1901.05335.","link":"/2020/05/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Adaptation/"},{"title":"注意力机制","text":"1. 简介2. SEnet (Squeeze-excitation network)3. SKNET, CBAM 等1. 简介定义：通过一定方式使得学习过程中仅仅关注部分信息的的手段都可以称作注意力机制，其可以让网络仅仅关注某些有用的信息，获取了关键信息就可以使用更加少的参数获得更加好的效果。 1.1 在 CV 中的应用方式： 我们知道图像中的信息就是特征图，那么对信息筛选就是对特征图进行筛选，这里的筛选可以在空间层面和维度层面，筛选的手段就是进行加权。 空间层面（Where ?）：使用维度为(BS,1,H,W)的加权向量，对不同空间位置的信息进行筛选 维度层面(What ?): 通常我们认为不同维度代表了不同卷积核的提取的特征，那么同一维度都是同一个特征提取器获得的，也就是输入特征图中所有有这个特征的位置。对不同维度的信息进行筛选就是采用维度为（BS，channels,1,1）的向量为每一特征图进行加权。 1.2 如何确定筛选权值: 目前我们知道注意力机制是通过加权的方式在空间和维度层面对特征图进行筛选，那么怎么确定权值呢？权值代表哪些信息重要，哪些信息不太关键，那么怎么来决定呢？这里显然需要根据任务来决定，当使用哪些信息我们获得更好的最终结果，哪些信息就是更关键的信息。 流程： 特征图生成权重：由于我们需要根据特征图判定重要性，那么我们建立一个以特征图为输入的浅层神经网络，其输出当作权值 权重作用于特征图：上一步生成的权值通过 broadcast 加权到特征图上，得到加权特征图 权重反馈：网络根据加权的特征图进行结果的预测，得到损失，进行反向传播，这里的损失就能让我们确定第一步生成的权重是否正确以及如何调整权重 循环1-3步就能获得越来越好的权重。 2. SEnet (Squeeze-excitation network)接下来我们来介绍一下基于 channel 维度进行注意力机制的 SEnet，这个视频讲解的比较清楚。 SEnet的流程主要分为三个部分： 压缩：对上一步的特征图进行全局池化，获得包含有全局感受野的特征向量。 激励：我们将这个特征向量传入两层 FC 的浅层神经网络获得权值 乘积：将权值乘到之前的每个特征图上就获得有不同注意力特征图 由于注意力机制是通过额外的神经网络进行支撑，因此我们可以将其加到任何的神经网络中，例如加到 Inception 和 Residual net的效果则如下图所示： 3. SKNET, CBAM 等见 SKnet 和 CBAM","link":"/2020/09/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"线性回归","text":"线性回归（基于神经网络+梯度下降）定义：基于特征和标签之间的线性函数关系约束，线性回归通过建立单层神经网络，将神经网络中每一个神经元当成是函数关系中的一个参数，通过利用初始输出和目标输出建立损失，并优化损失最小的方式使得神经元的数值和真实函数参数数值最相近，从而通过网络训练得到最符合数据分布的函数关系。 实施步骤： 初始通过随机化线性函数的参数，通过输入的x，会得到一系列y_h 输出的y_h和真实值y之间因为神经元参数不正确产生差距，为了y_h和y能尽量地逼近，我们通过平方误差损失函数（MSE Loss）来描述这种误差。 类似于通过求导得到损失函数最优解的方式，我们通过梯度下降法将这种误差传递到参数，通过调整参数使误差达到最小 通过几轮的训练，我们得到的最小的损失值对应的神经元数值，就是描述输入输出的线性关系的最好的参数。 要点： 确定输入输出之间一定满足线性关系，这一点可以通过对x,y画图看出，只有线性关系才能使用线性回归训练得到 由于线性关系唯一由神经元个数决定，不同的参数个数唯一决定了这种线性关系，因此需要选择适合的特征用于线性回归 最小二乘法（基于数据计算得到解析解的线性回归）参考shuhuai的视频 定义：最小二乘法是希望对 n 维平面的线性数据进行拟合得到输入输出的线性函数，其思想是建立一个线性模型 y = w_{1}x + ... + w_{k}x + b进行预测并使得预测的损失最小，其中损失函数为 L = \\sum_{i}(y_{i}-y_{i}) = \\sum_i(y_{i}-w^{T}x_{i})我们可以采用多种方式对这个损失函数进行优化 1. 矩阵推导出最小二乘解析解将损失函数化成矩阵表示，之后令损失函数对w求导得0，求得最优解，过程如下： 2. 利用函数求导方式求得最小二乘估计解析解除了表示成矩阵的形式，我们也可以直接对损失函数进行化简，求得使得损失函数最小的参数值，这个部分更容易推导 离差是一个凸函数 对凸函数里面的参数求导得到全局损失最小值对应的参数 3. 最小二乘估计的集合解释 最小二乘法实际上第一种形象的解释就是求出最能拟合数据点的直线，而最能拟合则使用数据到直线的离差最小的方式来表示 第二种集合解释则是从线性代数的角度出发： 将 N * p维的数据 X 想象成一个 p 维子空间的的基， 由于 y 不能完全由这 p 维数据线性表出，因此 y 则在这个 p 维子空间的外面， 而我们求得的 $y^{hat}$ 则是这个 p 维子空间里离 y 最近的向量，即 y 在子空间中的投影， 而损失则是 y 和投影之间的差距，即投影的法向量， 这样使得法向量最小，可以求得和矩阵表达求解析解里面相同的结果。 4. 概率角度的最小二乘估计当使用 y = w^{\\top}x + \\epsilon\\epsilon \\sim N(0,\\sigma^{2})表达输入输出的关系，并进一步利用此关系进行最大似然估计时，我们发现可以得到和上述相同的损失的表达式，这也就说明，当使用最大二乘估计时，其概率视角就是在噪声为高斯分布的基础上进行最大似然估计的结果。 这一节中出现的有用的函数 使用plt绘制散点图 12from matplotlib import pyplot as pltplt.scatter(features[:, 1].numpy(), labels.numpy(), 1) 自行制作dataLoader: dataloader 为输入dataset可以随机获取dataset中batch size 个样本 通过使用打乱的indices，每次yield batch size个样本，生成的生成器可以用for调用 参数初始化：自行初始化网络中的参数，使用init模块","link":"/2020/09/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"title":"test formula","text":"线性回归（基于神经网络+梯度下降）定义：基于特征和标签之间的线性函数关系约束，线性回归通过建立单层神经网络，将神经网络中每一个神经元当成是函数关系中的一个参数，通过利用初始输出和目标输出建立损失，并优化损失最小的方式使得神经元的数值和真实函数参数数值最相近，从而通过网络训练得到最符合数据分布的函数关系。 实施步骤： 初始通过随机化线性函数的参数，通过输入的x，会得到一系列y_h 输出的y_h和真实值y之间因为神经元参数不正确产生差距，为了y_h和y能尽量地逼近，我们通过平方误差损失函数（MSE Loss）来描述这种误差。 类似于通过求导得到损失函数最优解的方式，我们通过梯度下降法将这种误差传递到参数，通过调整参数使误差达到最小 通过几轮的训练，我们得到的最小的损失值对应的神经元数值，就是描述输入输出的线性关系的最好的参数。 要点： 确定输入输出之间一定满足线性关系，这一点可以通过对x,y画图看出，只有线性关系才能使用线性回归训练得到 由于线性关系唯一由神经元个数决定，不同的参数个数唯一决定了这种线性关系，因此需要选择适合的特征用于线性回归 最小二乘法（基于数据计算得到解析解的线性回归）参考shuhuai的视频 定义：最小二乘法是希望对 n 维平面的线性数据进行拟合得到输入输出的线性函数，其思想是建立一个线性模型 P(A_i \\mid B) = \\frac{P(B\\mid A)P(A_i)}{\\sum_{j=1}^{n}P(A_j)P(B \\mid A_j)} y = w_{1}x + ... + w_{k}x + b进行预测并使得预测的损失最小，其中损失函数为 L = \\sum_{i}(y_{i}-y_{i}) = \\sum_i(y_{i}-w^{T}x_{i})我们可以采用多种方式对这个损失函数进行优化 1. 矩阵推导出最小二乘解析解将损失函数化成矩阵表示，之后令损失函数对w求导得0，求得最优解，过程如下： 2. 利用函数求导方式求得最小二乘估计解析解除了表示成矩阵的形式，我们也可以直接对损失函数进行化简，求得使得损失函数最小的参数值，这个部分更容易推导 离差是一个凸函数 对凸函数里面的参数求导得到全局损失最小值对应的参数 3. 最小二乘估计的集合解释 最小二乘法实际上第一种形象的解释就是求出最能拟合数据点的直线，而最能拟合则使用数据到直线的离差最小的方式来表示 第二种集合解释则是从线性代数的角度出发： 将 N * p维的数据 X 想象成一个 p 维子空间的的基， 由于 y 不能完全由这 p 维数据线性表出，因此 y 则在这个 p 维子空间的外面， 而我们求得的 $y^{hat}$ 则是这个 p 维子空间里离 y 最近的向量，即 y 在子空间中的投影， 而损失则是 y 和投影之间的差距，即投影的法向量， 这样使得法向量最小，可以求得和矩阵表达求解析解里面相同的结果。 4. 概率角度的最小二乘估计当使用 y = w^{\\top}x + \\epsilon\\epsilon \\sim N(0,\\sigma^{2})表达输入输出的关系，并进一步利用此关系进行最大似然估计时，我们发现可以得到和上述相同的损失的表达式，这也就说明，当使用最大二乘估计时，其概率视角就是在噪声为高斯分布的基础上进行最大似然估计的结果。 这一节中出现的有用的函数 使用plt绘制散点图 12from matplotlib import pyplot as pltplt.scatter(features[:, 1].numpy(), labels.numpy(), 1) 自行制作dataLoader: dataloader 为输入dataset可以随机获取dataset中batch size 个样本 通过使用打乱的indices，每次yield batch size个样本，生成的生成器可以用for调用 123456789101112import torchimport randomdef data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # random read 10 samples for i in range(0, num_examples, batch_size): j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch yield features.index_select(0, j), labels.index_select(0, j) 参数初始化：自行初始化网络中的参数，使用init模块","link":"/2020/09/16/test/"}],"tags":[{"name":"字符串","slug":"字符串","link":"/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"表达式","slug":"表达式","link":"/tags/%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"论文阅读","slug":"论文阅读","link":"/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"综述","slug":"综述","link":"/tags/%E7%BB%BC%E8%BF%B0/"},{"name":"注意力机制","slug":"注意力机制","link":"/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"name":"最小二乘法","slug":"最小二乘法","link":"/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"}],"categories":[{"name":"cpp","slug":"cpp","link":"/categories/cpp/"},{"name":"见闻","slug":"见闻","link":"/categories/%E8%A7%81%E9%97%BB/"},{"name":"思考","slug":"思考","link":"/categories/%E6%80%9D%E8%80%83/"},{"name":"领域自适应","slug":"领域自适应","link":"/categories/%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]}