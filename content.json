{"pages":[{"title":"","text":"呀，一些碎碎念做一个自己的网站很酷呀，不过做完就去找工作了。（找工作倒是写了很多笔记，可以记录上来哈哈）但是也耽误了博客的更新，想起来可以记录些自己的想法，见闻，技术内容也挺好的。我一定好好更新嘻~~ 关于我：一个兜兜转转到CV的非典型程序媛 专业相关：目前在做计算机视觉相关的项目和研究，主要目标检测和语义分割做的多一些，目前在研究基于语义分割的领域自适应工作，争取出些成果（加油~） 业余爱好：喜欢各种运动（羽毛球，网球，足球，篮球，都很喜欢）；日日离不开耳机；一看美剧就人间消失；看展看演出积极分子； 经历：焊板子，单片机编程，matlab仿真，计算机视觉都做过哈哈，虽然前几项已经忘得差不多了。","link":"/about/index.html"}],"posts":[{"title":"领域自适应综述","text":"温故知新篇 – 领域自适应中的多种方法和内部含义浅析本文的行文结构如下，首先从定义出发描述，继而通过现有问题和数学模型开始理解什么是领域自适应问题。基于理解，我们罗列现有的几大主流领域自适应方法，并使用我们的数学模型辅助理解，提出笔者的一些思考。 领域自适应定义：现有深度学习模型使用源域的知识，并运用一系列领域自适应方法，提升其在目标域上的表现。基于研究领域自适应问题本质为研究泛化问题，其有众多的应用方向，包括但不限于分类，目标检测，语义分割等。 如何理解领域自适应： 首先，我们可以来看一下，什么样的情况会导致领域不适应；接着，我们来使用自己建立的数学模型来理解领域不适应时到底发生了什么；最后我们引出，基于这个数学模型，我们如何来理解现有的多种领域自适应方法。 领域不适应的情形：领域不适应，即神经网络的泛化性能较差，其在以下情形容易发生： 神经网络模型对于训练集分布过拟合：网络在训练集上的指标和测试集上指标差异达到5%及以上，并且随着训练的进行差异越来越大。 测试集分布和训练集分布差异较大：我们发现，在生成图像上训练的模型，在真实图片上训练效果很差。 这时，我们会说神经网络在测试集上的表现性能差，我们称这个神经网络模型的泛化性能差。那么，如何从数学模型的角度来理解泛化的问题呢？ 数学模型解释泛化： 我们不妨先假设我们在进行分类问题，同时我们将数据的分布降为两维，而不是图像的千万维，那么分类问题即在二维平面上寻找一条分界线，将两个不同的标记的类别进行区分。 神经网络对于训练集过拟合：这个情形我们应当很熟悉了，此时分界线在训练集的不同类别之间交叉穿过，但是由于测试集的分布和训练集不同，因此产生了大量的错误分类。这时，我们的问题在于分界线的复杂度太高，超过了数据本身真实分布需要的分界线，或者可以说模型的复杂度和真实分界线的复杂度之间有维度差距。 测试集和训练集分布差异大：这时我们可以想象一个适当复杂度的分界线对训练集的不同类别进行分类，但是测试集和训练集的分布有差异。同样的类别，但是产生了分布偏移，甚至和原来的类别差异较远。说明当前用于训练神经网络模型的数据分布不能反映真实数据的分布。从而生成的分界线不能正确分割真实数据。 从以上分析我们可以看出，领域自适应的问题，本质就是数据分布和分界线的问题，一个良好的领域自适应方法应当同时兼顾对齐源域和目标域的数据分布，同时对分界线进行约束。从而源域和目标域的数据分布对齐良好，同时模型的分界边界远离数据分布并能泛化到其他相同分布数据。 现有的多种解决领域自适应的方法： 基于A review of domain adaptation without target labels [1] 这篇综述，我们按照下面三个方向进行分类： 基于采样的方法: 给予不同样本不同的权重，促使源域中分布和目标域不一致的部分尽量少地影响神经网络。这类方法不能对某一类数据的分布进行变换（平移，聚集等），从而不能优化源域和目标域数据同一类别特征有较大差异带来的影响，而只能通过改变分布的存在与否来间接地影响分类边界。 基于特征对齐的方法: 将源域和目标域的神经网络特征分布进行对齐，从而同样类别的物体能在特征分布空间中尽量地接近。这类方法应当是最灵活的，但是我们需要找到一个合适的方式来实现特征对齐。 基于推理的方法：通过在优化过程中加上额外限制的方式，从而估计神经网络的模型参数，促使神经网络学习的分界线满足限制。这类方法可以看成是对分界线的变换下面我们通过独立的模块，细致地描述这些方法 如何做到领域自适应做到领域自适应，关键在于如何基于源域数据上学习到的分界线信息，加上源域和目标域之间的差异信息，促使数据差异减小，分界线向更适应目标域的方向移动。 1. 基于采样的方法：基于采样的方法目的在于消除采集源域数据时的偏差带给神经网络的影响，我们希望采样的数据分布和真实的数据分布尽量一致，从而提升神经网络对真实数据的适应性。现代的数据方法往往通过预估真实分布中每个实例出现的比例来进行采样后分布的校正，但是在真实场景的数据往往没那么好估计。 数据层次重要性权重 类别层次重要性权重2. 基于特征对齐的方法： 子空间映射 最佳传输 域不变空间匹配 深度特征匹配 协同学习（Correspondence learning） 参考文献： Kouw, W.M., &amp; Loog, M. (2019). A review of single-source unsupervised domain adaptation. ArXiv, abs/1901.05335.","link":"/2020/05/06/%E4%B8%93%E4%B8%9A%E7%9B%B8%E5%85%B3/Adaptation/"},{"title":"思考","text":"","link":"/2020/09/09/%E6%97%A5%E6%9C%89%E6%89%80%E6%80%9D/post/"},{"title":"见闻","text":"","link":"/2020/09/09/%E6%89%80%E8%A7%81%E6%89%80%E9%97%BB/post/"},{"title":"线性回归","text":"线性回归（基于神经网络+梯度下降）定义：基于特征和标签之间的线性函数关系约束，线性回归通过建立单层神经网络，将神经网络中每一个神经元当成是函数关系中的一个参数，通过利用初始输出和目标输出建立损失，并优化损失最小的方式使得神经元的数值和真实函数参数数值最相近，从而通过网络训练得到最符合数据分布的函数关系。 实施步骤： 初始通过随机化线性函数的参数，通过输入的x，会得到一系列y_h 输出的y_h和真实值y之间因为神经元参数不正确产生差距，为了y_h和y能尽量地逼近，我们通过平方误差损失函数（MSE Loss）来描述这种误差。 类似于通过求导得到损失函数最优解的方式，我们通过梯度下降法将这种误差传递到参数，通过调整参数使误差达到最小 通过几轮的训练，我们得到的最小的损失值对应的神经元数值，就是描述输入输出的线性关系的最好的参数。 要点： 确定输入输出之间一定满足线性关系，这一点可以通过对x,y画图看出，只有线性关系才能使用线性回归训练得到 由于线性关系唯一由神经元个数决定，不同的参数个数唯一决定了这种线性关系，因此需要选择适合的特征用于线性回归 最小二乘法（基于数据计算得到解析解的线性回归）参考shuhuai的视频 定义：最小二乘法是希望对 n 维平面的线性数据进行拟合得到输入输出的线性函数，其思想是建立一个线性模型 $$y = w_{1}x + … + w_{k}x + b $$ 进行预测并使得预测的损失最小，其中损失函数为 $$L = \\sum_{i}(y_{i}-y_{i}) = \\sum_i(y_{i}-w^{T}x_{i}) $$ 我们可以采用多种方式对这个损失函数进行优化 1. 矩阵推导出最小二乘解析解将损失函数化成矩阵表示，之后令损失函数对w求导得0，求得最优解，过程如下： 2. 利用函数求导方式求得最小二乘估计解析解除了表示成矩阵的形式，我们也可以直接对损失函数进行化简，求得使得损失函数最小的参数值，这个部分更容易推导 离差是一个凸函数 对凸函数里面的参数求导得到全局损失最小值对应的参数 3. 最小二乘估计的集合解释 最小二乘法实际上第一种形象的解释就是求出最能拟合数据点的直线，而最能拟合则使用数据到直线的离差最小的方式来表示 第二种集合解释则是从线性代数的角度出发： 将 N * p维的数据 X 想象成一个 p 维子空间的的基， 由于 y 不能完全由这 p 维数据线性表出，因此 y 则在这个 p 维子空间的外面， 而我们求得的 $y^{hat}$ 则是这个 p 维子空间里离 y 最近的向量，即 y 在子空间中的投影， 而损失则是 y 和投影之间的差距，即投影的法向量， 这样使得法向量最小，可以求得和矩阵表达求解析解里面相同的结果。 4. 概率角度的最小二乘估计当使用$$y = w^{\\top}x + \\epsilon $$, $$\\epsilon \\sim N(0,\\sigma^{2})$$ 表达输入输出的关系，并进一步利用此关系进行最大似然估计时，我们发现可以得到和上述相同的损失的表达式，这也就说明，当使用最大二乘估计时，其概率视角就是在噪声为高斯分布的基础上进行最大似然估计的结果。 这一节中出现的有用的函数 使用plt绘制散点图 12from matplotlib import pyplot as pltplt.scatter(features[:, 1].numpy(), labels.numpy(), 1) 自行制作dataLoader: dataloader 为输入dataset可以随机获取dataset中batch size 个样本 通过使用打乱的indices，每次yield batch size个样本，生成的生成器可以用for调用 123456789101112import torchimport randomdef data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # random read 10 samples for i in range(0, num_examples, batch_size): j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch yield features.index_select(0, j), labels.index_select(0, j) 参数初始化：自行初始化网络中的参数，使用init模块 12345from torch.nn import initinit.normal_(net[0].weight, mean=0.0, std=0.01)init.constant_(net[0].bias, val=0.0) 重要的问题：1.构建一个深度学习网络并训练需要哪些步骤？深度学习网络的主要组成部分就是数据，网络和训练，因此可以根据这三部分展开为下面几个步骤： 0.数据部分 生成数据集/找到现有数据集 根据数据集构建Dataset 并用之构建dataloader （可选）调用构建的Dataloader，得到数据并可视化，检查实现的正确性，并对数据有一定了解 1.网络部分4. 定义模型，初始化模型参数5. 定义损失函数，如本节的MSE loss6. 定义优化函数，SGD,Adam… 及其参数：学习率，动量，weight_decay… 2.训练部分7. 使用循环的方式，每个循环训练一遍所有数据8. 将数据输入网络，根据损失函数和网络输出建立损失9. 梯度清零，损失回传，优化器更新损失10. 记录损失，可视化结果，往复训练 2.什么时候该用parameter.data?下面是课程中使用的优化器的代码，可以发现，参数的更新使用了param.data 1234def sgd(params, lr, batch_size): for param in params: param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track 根据我的理解，这是由于反向传播机制在需要更新的参数进行运算时会构建动态运算图，如果直接使用这个param进行更新，就会在动态图中计入这一部分，从而反向传播时也会将这一步运算的梯度加入。而我们实际希望的则是损失函数对参数进行求导，而不希望再此参数上“节外生枝”。因此，在网络前向传播和损失函数计算之外的参数运算，应当使用param.data进行更新","link":"/2020/09/09/%E4%B8%93%E4%B8%9A%E7%9B%B8%E5%85%B3/post/"}],"tags":[{"name":"综述","slug":"综述","link":"/tags/%E7%BB%BC%E8%BF%B0/"}],"categories":[{"name":"专业相关","slug":"专业相关","link":"/categories/%E4%B8%93%E4%B8%9A%E7%9B%B8%E5%85%B3/"}]}